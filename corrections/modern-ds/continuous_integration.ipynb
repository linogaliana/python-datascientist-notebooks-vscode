{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Intégration continue avec Python\n",
        "\n",
        "Lino Galiana  \n",
        "2025-03-19\n",
        "\n",
        "**Cette page sera actualisée prochainement, une version plus à jour et plus complète peut être trouvée sur <https://ensae-reproductibilite.github.io/website/>**\n",
        "\n",
        "L’un des apports principaux des innovations\n",
        "récentes de la *data science* est la\n",
        "manière dont des projets, malgré\n",
        "leur complexité, peuvent facilement\n",
        "être converti en projets pérennes\n",
        "à partir\n",
        "d’un prototype bien construit.\n",
        "En s’inspirant de l’approche `devops` ,\n",
        "méthode de travail qui consiste à adopter un certain\n",
        "nombre de gestes pour\n",
        "automatiser la production de livrables ou de tests\n",
        "dès la\n",
        "conception du produit, les *data scientists*\n",
        "ont adopté une méthode de travail très efficace\n",
        "pour favoriser la réutilisation de leur travail\n",
        "par d’autres équipes que celles à l’origine de\n",
        "la conception du protype initial.\n",
        "\n",
        "Cette approche `devops` a été reprise et étendue\n",
        "pour donner un autre buzz-word, le `MLops`.\n",
        "Il s’agit d’une approche qui vise à créer\n",
        "et mettre à disposition des modèles de machine\n",
        "learning de manière fiable et automatisée\n",
        "à chaque nouvelle étape du projet, en parallèle\n",
        "de la mise à jour du code ayant produit ces\n",
        "output.\n",
        "\n",
        "Ces nouvelles méthodes de travail permettent\n",
        "des gains substantiels de productivité\n",
        "pour les équipes développant des modèles\n",
        "et réduit fortement le coût de reprise d’un\n",
        "code par une équipe en charge de sa\n",
        "pérenisation. Ce coût est en effet le principal\n",
        "frein à la mise en production de nouveaux\n",
        "projets ce qui peut représenter un gâchis\n",
        "non négligeable de temps et de ressources.\n",
        "Comme nous l’expliquons avec Romain Avouac\n",
        "dans un cours de dernière année de l’ENSAE\n",
        "(https://ensae-reproductibilite.github.io/website/),\n",
        "l’adoption de certaines bonnes pratiques\n",
        "de développement de code et d’une démarche\n",
        "exploitant les dernières innovations de\n",
        "la *data science* peut substantiellement\n",
        "augmenter les chances d’un succès\n",
        "d’un projet. Le nouveau paradigme, qui\n",
        "consiste à intégrer en amont du projet\n",
        "certaines contraintes de la production\n",
        "et tester continuellement la manière dont les\n",
        "livrables évoluent, évite que la mise\n",
        "en production d’un projet, qui est coûteuse\n",
        "en temps et en ressources, n’aboutisse qu’au\n",
        "moment où le projet est déjà caduc\n",
        "(car les données ou les besoins ont évolués…).\n",
        "\n",
        "# 1. L’intégration continue: une opportunité pour les *data scientists*\n",
        "\n",
        "On retrouve régulièrement l’acronyme CI/CD\n",
        "pour illustrer cette\n",
        "nouvelle méthode de travail dans le\n",
        "monde du développement logiciel :\n",
        "\n",
        "-   l’intégration continue (CI pour\n",
        "    *continuous integration*)\n",
        "    est une pratique consistant, de manière automatique,\n",
        "    à fréquemment tester les effets d’une modification faite à un code ou à un\n",
        "    document faisant parti d’un projet informatique.\n",
        "\n",
        "-   le déploiement en continu (CD pour *continuous\n",
        "    delivery*) consiste à intégrer de manière automatisée\n",
        "    la production d’un ou plusieurs livrables (environnement\n",
        "    portable, application, site web, etc.) à chaque\n",
        "    modification du code associé à un projet informatique.\n",
        "\n",
        "Cette pratique permet ainsi de détecter de manière précoce des possibilités\n",
        "de *bug* ou l’introduction d’un changement non anticipé. Tout comme `Git`,\n",
        "cette pratique devient un standard dans les domaines collaboratifs.\n",
        "\n",
        "L’intégration continue permet de sécuriser le travail, puisqu’elle offre un\n",
        "filet de sécurité (par exemple un test sur une machine à la configuration\n",
        "arbitraire), mais permet aussi de déployer en temps réel certaines\n",
        "évolutions. On parle parfois de déploiement en continu, complémentaire de\n",
        "l’intégration continue. Cette approche réduit ainsi\n",
        "la muraille de Chine entre un\n",
        "analyste de données et une équipe de développeurs d’application. Elle offre donc\n",
        "plus de contrôle, pour le producteur d’une analyse statistique, sur la\n",
        "valorisation de celle-ci.\n",
        "\n",
        "Cette approche consiste une excellente opportunité\n",
        "pour les *data scientists* d’être en mesure\n",
        "de valoriser leurs projets auprès de publics aux\n",
        "exigences différentes. Pour des développeurs, le\n",
        "*data scientist* pourra fournir une image `Docker`\n",
        "(environnement portable où l’ensemble des dépendances\n",
        "et des configurations systèmes pour faire tourner un code\n",
        "sont contrôlés) permettant à d’autres d’exécuter\n",
        "facilement le code d’un projet. Pour faciliter\n",
        "la réutilisation d’un modèle par d’autres *data scientists*,\n",
        "il devient de plus en plus fréquent d’exposer\n",
        "un modèle sous forme d’API: les personnes désirant\n",
        "réutiliser le modèle peuvent directement l’appliquer\n",
        "en accédant à une prédiction par le biais d’une API\n",
        "ce qui évite d’avoir à fournir le jeu d’entraînement\n",
        "si ce dernier est sensible. Pour toucher\n",
        "des publics moins\n",
        "familiers du code, la mise à disposition de sites web\n",
        "interactifs valorisant certains résultats d’un projet\n",
        "peut être intéressante. Cette approche très exigeante\n",
        "d’utiliser un même projet pour toucher des cibles\n",
        "très différentes est grandement facilitée par le\n",
        "déploiement en continu et la mise à disposition\n",
        "de librairies ou d’infrastructures\n",
        "dédiées dans le monde de l’*open-source*.\n",
        "\n",
        "Tout en restant éco-responsable (voir partie XXX), cela\n",
        "permet de mieux valoriser des projets pour réduire\n",
        "les coûts à le maintenir et le faire évoluer.\n",
        "Le cours de dernière année de l’ENSAE que je développe\n",
        "avec Romain Avouac ([ensae-reproductibilite.github.io/](https://ensae-reproductibilite.github.io/website/))\n",
        "présente beaucoup plus de détails sur cette question.\n",
        "\n",
        "# 2. L’intégration continue en pratique\n",
        "\n",
        "L’intégration continue fonctionne très bien sur `Gitlab` et sur `Github`.\n",
        "A chaque interaction avec le dépôt distant (`push`), une série d’instruction\n",
        "définie par l’utilisateur est exécutée. `Python` et `R` s’intègrent très bien dans ce paradigme grâce\n",
        "à un certain nombre d’images de base (concept sur lequel nous allons revenir)\n",
        "qui peuvent être customisées pour répondre à une certaine configuration\n",
        "nécessaire pour exécuter des codes.\n",
        "C’est une méthode idéale pour améliorer la reproductibilité d’un projet: les\n",
        "instructions exécutées le sont dans un environnement isolé et contrôlé, ce qui\n",
        "diffère d’une machine personnelle.\n",
        "\n",
        "# 3. Comment fonctionne l’intégration continue ?\n",
        "\n",
        "L’intégration continue repose sur le système de la *dockerisation* ou *conteneurisation*.\n",
        "La technologie sous jacente s’appelle `Docker`.\n",
        "Il s’agit d’une technologie qui permet la construction\n",
        "de machines autosuffisantes\n",
        "(que l’on nomme **containeurs**) répliquant un environnement\n",
        "contrôlé (que l’on nomme **image**).\n",
        "\n",
        "On parle de *pipelines* pour désigner une suite de tâches pour partir de 0\n",
        "(généralement une machine `Linux` à la configuration minimale) et aboutir\n",
        "à l’issue d’une série d’instructions définies par l’utilisateur.\n",
        "\n",
        "L’objectif est de trouver une image la plus\n",
        "parcimonieuse possible, c’est-à-dire à la configuration minimale, qui permet\n",
        "de faire tourner le code voulu.\n",
        "Les [Actions Github](https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-python)\n",
        "consistuent un modèle sur lequel il est facile\n",
        "de s’appuyer lorsqu’on a des connaissances limitées\n",
        "concernant \\`Docker.\n",
        "Il est également très simple de construire son image\n",
        "de rien, ce qui est la démarche choisie dans\n",
        "l’autre cours de l’ENSAE que nous donnons avec Romain\n",
        "Avouac (https://ensae-reproductibilite.github.io/website/).\n",
        "\n",
        "Quand on utilise un dépôt `Github` <i class=\"fa-brands fa-github\"></i>\n",
        "ou `Gitlab` <i class=\"fa-brands fa-gitlab\"></i>,\n",
        "des services automatiques\n",
        "d’intégration continue peuvent être utilisés :\n",
        "\n",
        "-   `Gitlab CI`: solution pleinement intégrée à un dépôt `Gitlab`. Très généraliste\n",
        "    et permettant des *pipelines* très complexes\n",
        "    ([voir l’intégration continue du projet utilitR, une documentation pour R](https://gitlab.com/linogaliana/documentationR/-/blob/main/.gitlab-ci.yml)).\n",
        "    Il est également possible de\n",
        "    l’utiliser avec un dépôt stocké sur `Github`. L’inconvénient de cette approche\n",
        "    est qu’elle est assez lente.\n",
        "-   `Github Actions`: c’est l’alternative (relativement récente) au service d’intégration continue de\n",
        "    Gitlab uniquement basée sur les technologies `Github`. La très forte\n",
        "    dynamique de développement a rendu ce service incontournable.\n",
        "    Un grand nombre de scripts pré-définis et paramétrables\n",
        "    facilitent l’entrée dans le monde de l’intégration\n",
        "    continue.\n",
        "\n",
        "Historiquement, il existait d’autres services d’intégration continue, notamment\n",
        "`Travis CI` ou `AppVeyor`[1]\n",
        "\n",
        "## 3.1 Fonctionnement des actions Github\n",
        "\n",
        "Les actions `Github` fonctionnent par couches successives au sein desquelles\n",
        "on effectue un certain nombre d’instructions.\n",
        "La meilleure manière d’apprendre les actions `Github` est, certes, de [lire la\n",
        "documentation officielle](https://docs.github.com/en/actions) mais surtout,\n",
        "à mon avis, de regarder quelques *pipelines* pour comprendre la démarche.\n",
        "\n",
        "L’un des intérêts des `Github Actions` est la possibilité d’avoir un *pipeline*\n",
        "proposant une intrication de langages différents pour avoir une chaine de\n",
        "production qui propose les outils les plus efficaces pour répondre à un\n",
        "objectif en limitant les verrous techniques.\n",
        "\n",
        "Par exemple, le *pipeline* de ce cours, disponible\n",
        "sur `Github` propose une intrication des langages\n",
        "`Python` et `R` avec des technologies `Anaconda` (pour contrôler\n",
        "l’environnement `Python` comme expliqué dans les chapitres précédents)\n",
        "et `Javascript` (pour le déploiement d’un site web avec le service tiers\n",
        "`Netlify`)[2]. Cette chaîne de production multi-langage permet que\n",
        "les mêmes fichiers sources génèrent un site web et des notebooks disponibles\n",
        "sur plusieurs environnements.\n",
        "\n",
        "[1] Ces services d’intégration continue étaient utilisés lorsque `Github`\n",
        "ne proposait pas encore de service intégré, comme le faisait `Gitlab`.\n",
        "Ils sont de moins en moins fréquemment utilisés.\n",
        "\n",
        "[2] Pour réduire le temps nécessaire pour construire le site *web*, ce\n",
        "*pipeline* s’appuie sur un environnement `Docker` construit sur un autre dépôt\n",
        "disponible également sur `Github`\n",
        "<a href=\"https://github.com/linogaliana/python-datascientist-docker/blob/main/.github/workflows/prod.yml\" class=\"github\"><i class=\"fab fa-github\"></i></a>.\n",
        "Celui-ci part d’une configuration système `Linux` et construit un environnement\n",
        "`Anaconda` à partir d’un fichier `environment.yml` qui liste toutes les dépendances\n",
        "nécessaires pour exécuter les morceaux de code du site *web*.\n",
        "Cet environnement `Anaconda` est construit grâce à l’outil `mamba` qui permet\n",
        "d’aller beaucoup plus vite dans la constitution d’environnements que ne le\n",
        "permet `conda`."
      ],
      "id": "7ccffe7a-dc13-4336-a9d1-dd8404020beb"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name: Production deployment\n",
            "\n",
            "on:\n",
            "  push:\n",
            "    branches:\n",
            "      - main\n",
            "      - master\n",
            "      - test-uv-again\n",
            "\n",
            "jobs:\n",
            "  docker:\n",
            "    runs-on: ubuntu-latest\n",
            "    steps:\n",
            "      -\n",
            "        name: Set up QEMU\n",
            "        if: ${{ github.repository == 'linogaliana/python-datascientist' }}\n",
            "        uses: docker/setup-qemu-action@v3\n",
            "      -\n",
            "        name: Set up Docker Buildx\n",
            "        if: ${{ github.repository == 'linogaliana/python-datascientist' }}\n",
            "        uses: docker/setup-buildx-action@v3\n",
            "      -\n",
            "        name: Login to DockerHub\n",
            "        if: ${{ github.repository == 'linogaliana/python-datascientist' }}\n",
            "        uses: docker/login-action@v3\n",
            "        with:\n",
            "          username: ${{ secrets.DOCKERHUB_USERNAME }}\n",
            "          password: ${{ secrets.DOCKERHUB_TOKEN }}\n",
            "      -\n",
            "        name: Build and push\n",
            "        if: ${{ github.repository == 'linogaliana/python-datascientist' }}\n",
            "        id: docker_build\n",
            "        uses: docker/build-push-action@v6\n",
            "        env:\n",
            "          GITHUB_PAT: ${{ secrets.PAT }}\n",
            "        with:\n",
            "          file: \"docker/Dockerfile\"\n",
            "          push: true\n",
            "          tags: linogaliana/python-datascientist:latest\n",
            "      -\n",
            "        name: Image digest\n",
            "        run: echo ${{ steps.docker_build.outputs.digest }}\n",
            "  pages:\n",
            "    name: Render-Blog\n",
            "    runs-on: ubuntu-latest\n",
            "    container: linogaliana/python-datascientist:latest\n",
            "    needs: docker\n",
            "    if: ${{ !github.event.pull_request.head.repo.fork }}\n",
            "    steps:\n",
            "      - uses: actions/checkout@v4\n",
            "        with:\n",
            "          fetch-depth: 0\n",
            "          ref: ${{ github.event.pull_request.head.ref }}\n",
            "          repository: ${{github.event.pull_request.head.repo.full_name}}\n",
            "      - name: Configure safe.directory  # Workaround for actions/checkout#760\n",
            "        run: git config --global --add safe.directory /__w/python-datascientist/python-datascientist\n",
            "      - name: Render website\n",
            "        env:\n",
            "          API_INPI_USERNAME: ${{ secrets.API_INPI_USERNAME }}\n",
            "          API_INPI_PASSWORD: ${{ secrets.API_INPI_PASSWORD }}\n",
            "        run: |\n",
            "          rm _quarto.yml\n",
            "          cp _quarto-prod.yml _quarto.yml\n",
            "          python build/append_environment.py\n",
            "          quarto render --profile fr --to html\n",
            "          python build/sidebar.py --to english\n",
            "          quarto render --profile en --to html\n",
            "          python build/sidebar.py --to french\n",
            "      - name: Archive build as artifacts\n",
            "        uses: actions/upload-artifact@v4\n",
            "        with:\n",
            "          name: sitedir\n",
            "          path: |\n",
            "            _site\n",
            "      - name: Publish to Pages\n",
            "        if: github.ref == 'refs/heads/main'\n",
            "        run: |\n",
            "          git config --global user.email quarto-github-actions-publish@example.com\n",
            "          git config --global user.name \"Quarto GHA Workflow Runner\"\n",
            "          quarto publish gh-pages . --no-render --no-browser\n",
            "  enonces:\n",
            "    name: Render notebooks\n",
            "    runs-on: ubuntu-latest\n",
            "    container: linogaliana/python-datascientist:latest\n",
            "    needs: docker\n",
            "    if: ${{ !github.event.pull_request.head.repo.fork }}\n",
            "    steps:\n",
            "      - uses: actions/checkout@v4\n",
            "        with:\n",
            "          fetch-depth: 0\n",
            "          ref: ${{ github.event.pull_request.head.ref }}\n",
            "      - name: Configure safe.directory  # Workaround for actions/checkout#760\n",
            "        run: |\n",
            "          git config --global --add safe.directory /__w/python-datascientist/python-datascientist\n",
            "          git config --global --add safe.directory /__w/python-datascientist/python-datascientist-notebooks\n",
            "      - shell: bash\n",
            "        run: |\n",
            "          ls\n",
            "          conda info\n",
            "          conda list\n",
            "      - name: Convert in ipynb with Quarto\n",
            "        env:\n",
            "          API_INPI_USERNAME: ${{ secrets.API_INPI_USERNAME }}\n",
            "          API_INPI_PASSWORD: ${{ secrets.API_INPI_PASSWORD }}\n",
            "        run: |\n",
            "          export QUARTO_PROFILE=fr,en\n",
            "          rm _quarto.yml\n",
            "          cp _quarto-prod.yml _quarto.yml\n",
            "          rm content/modelisation/index.qmd # Remove file not building in ipynb\n",
            "          quarto render --profile fr --to ipynb\n",
            "          quarto render --profile en --to ipynb\n",
            "      - name: Move to expected directory\n",
            "        env:\n",
            "          API_INPI_USERNAME: ${{ secrets.API_INPI_USERNAME }}\n",
            "          API_INPI_PASSWORD: ${{ secrets.API_INPI_PASSWORD }}\n",
            "        run: |\n",
            "          mkdir -p temp_notebooks\n",
            "          mkdir -p temp_notebooks/notebooks\n",
            "          python build/move_files.py --direction temp_notebooks/notebooks\n",
            "          quarto render content --to ipynb --execute -M echo:true\n",
            "          mkdir -p temp_notebooks/corrections\n",
            "          python build/move_files.py --direction temp_notebooks/corrections\n",
            "      - uses: actions/upload-artifact@v4\n",
            "        with:\n",
            "          name: Source enonce\n",
            "          path: content/\n",
            "      - uses: actions/upload-artifact@v4\n",
            "        with:\n",
            "          name: Enonces\n",
            "          path: temp_notebooks/notebooks/\n",
            "      - name: Pushes to another repository\n",
            "        uses: linogaliana/github-action-push-to-another-repository@main\n",
            "        env:\n",
            "          API_TOKEN_GITHUB: ${{ secrets.API_TOKEN_GITHUB }}\n",
            "        with:\n",
            "          source-directory: 'temp_notebooks/'\n",
            "          destination-repository-username: 'linogaliana'\n",
            "          destination-repository-name: 'python-datascientist-notebooks'\n",
            "          user-email: lino.galiana@insee.fr\n",
            "          destination-github-username: linogaliana\n",
            "          #target-branch: test\n",
            "          create-target-branch-if-needed: true\n",
            "          reset-repo: true\n",
            "  enonces-vscode:\n",
            "    name: Render notebooks (VSCode version)\n",
            "    runs-on: ubuntu-latest\n",
            "    container: linogaliana/python-datascientist:latest\n",
            "    needs: docker\n",
            "    if: ${{ !github.event.pull_request.head.repo.fork }}\n",
            "    steps:\n",
            "      - uses: actions/checkout@v4\n",
            "        with:\n",
            "          fetch-depth: 0\n",
            "          ref: ${{ github.event.pull_request.head.ref }}\n",
            "      - name: Configure safe.directory  # Workaround for actions/checkout#760\n",
            "        run: |\n",
            "          git config --global --add safe.directory /__w/python-datascientist/python-datascientist\n",
            "          git config --global --add safe.directory /__w/python-datascientist/python-datascientist-notebooks\n",
            "      - shell: bash\n",
            "        run: |\n",
            "          ls\n",
            "          conda info\n",
            "          conda list\n",
            "      - name: Convert in ipynb with Quarto\n",
            "        env:\n",
            "          API_INPI_USERNAME: ${{ secrets.API_INPI_USERNAME }}\n",
            "          API_INPI_PASSWORD: ${{ secrets.API_INPI_PASSWORD }}\n",
            "        run: |\n",
            "          export QUARTO_PROFILE=fr,en\n",
            "          rm _quarto.yml\n",
            "          python ./build/nice-vscode/tweak_pipeline_vscode.py --filename _quarto-prod.yml\n",
            "          cp _quarto-prod2.yml _quarto.yml\n",
            "          rm content/modelisation/index.qmd # Remove file not building in ipynb\n",
            "          quarto render --profile fr --to ipynb\n",
            "          quarto render --profile en --to ipynb\n",
            "      - name: Move to expected directory\n",
            "        env:\n",
            "          API_INPI_USERNAME: ${{ secrets.API_INPI_USERNAME }}\n",
            "          API_INPI_PASSWORD: ${{ secrets.API_INPI_PASSWORD }}\n",
            "        run: |\n",
            "          mkdir -p temp_notebooks\n",
            "          mkdir -p temp_notebooks/notebooks\n",
            "          python build/move_files.py --direction temp_notebooks/notebooks\n",
            "          quarto render content --to ipynb --execute -M echo:true\n",
            "          mkdir -p temp_notebooks/corrections\n",
            "          python build/move_files.py --direction temp_notebooks/corrections\n",
            "      - name: Pushes to another repository\n",
            "        uses: linogaliana/github-action-push-to-another-repository@main\n",
            "        env:\n",
            "          API_TOKEN_GITHUB: ${{ secrets.API_TOKEN_GITHUB }}\n",
            "        with:\n",
            "          source-directory: 'temp_notebooks/'\n",
            "          destination-repository-username: 'linogaliana'\n",
            "          destination-repository-name: 'python-datascientist-notebooks-vscode'\n",
            "          user-email: lino.galiana@insee.fr\n",
            "          destination-github-username: linogaliana\n",
            "          #target-branch: test\n",
            "          create-target-branch-if-needed: true\n",
            "          reset-repo: true\n",
            "\n"
          ]
        }
      ],
      "source": [],
      "id": "7cf8e36b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Les couches qui constituent les étapes du *pipeline*\n",
        "portent ainsi le nom de `steps`. Un *step* peut comporter un certain\n",
        "nombre d’instructions ou exécuter des instructions pré-définies.\n",
        "L’une de ces instructions prédéfinies est, par exemple,\n",
        "l’[installation de Python](https://github.com/actions/setup-python)\n",
        "ou l’[initialisation d’un environnement conda](https://github.com/marketplace/actions/setup-miniconda).\n",
        "La documentation officielle de `Github` propose un\n",
        "[fichier qui peut servir de modèle](https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-nodejs-or-python?langId=py)\n",
        "pour tester un script `Python` voire l’uploader de manière automatique\n",
        "sur `Pypi`.\n",
        "\n",
        "## 3.2 Intégration continue avec `Python`: tester un notebook\n",
        "\n",
        "Cette section n’est absolument pas exhaustive. Au contraire, elle ne fournit\n",
        "qu’un exemple minimal pour expliquer la logique de l’intégration continue. Il\n",
        "ne s’agit ainsi pas d’une garantie absolue de reproductibilité d’un *notebook*.\n",
        "\n",
        "`Github` propose une action officielle pour utiliser `Python` dans un\n",
        "*pipeline* d’intégration continue. Elle est disponible sur le\n",
        "[MarketPlace Github](https://github.com/marketplace/actions/setup-python).\n",
        "Il s’agit d’un bon point de départ, à enrichir.\n",
        "\n",
        "Le fichier qui contrôle les instructions exécutées dans l’environnement `Actions`\n",
        "doit se trouver dans le dossier `.github/workflows/`\n",
        "(:warning: ne pas oublier le point au début du\n",
        "nom du dossier). Il doit être au format `YAML` avec une extension `.yml`\n",
        "ou `.yaml`.\n",
        "Il peut avoir n’importe quel nom néanmoins il\n",
        "vaut mieux lui donner un nom signifiant,\n",
        "par exemple `prod.yml` pour un fichier contrôlant une chaîne de production.\n",
        "\n",
        "### 3.2.1 Lister les dépendances\n",
        "\n",
        "Avant d’écrire les instructions à exécuter par `Github`, il faut définir un\n",
        "environnement d’exécution car `Github` ne connaît pas la configuration `Python`\n",
        "dont vous avez besoin.\n",
        "\n",
        "Il convient ainsi de lister les dépendances nécessaires dans un fichier\n",
        "`requirements.txt` (si on utilise un environnement virtuel)\n",
        "ou un fichier `environment.yml` (si on préfère\n",
        "utiliser un environnement conda).\n",
        "Bien que le principe sous-jacent soit légèrement différent,\n",
        "ces fichiers ont la même fonction :\n",
        "permettre la création d’un environnement *ex-nihilo*\n",
        "avec un certain nombre de dépendances pré-installées[1].\n",
        "\n",
        "Si on fait le choix de l’option `environment.yml`,\n",
        "le fichier prendra ainsi la forme\n",
        "suivante, à enrichir en fonction de la\n",
        "richesse de l’environnement souhaité. :\n",
        "\n",
        "``` yaml\n",
        "channels:\n",
        "  - conda-forge\n",
        "\n",
        "dependencies:\n",
        "  - python>=3.10\n",
        "  - jupyter\n",
        "  - jupytext\n",
        "  - matplotlib\n",
        "  - nbconvert\n",
        "  - numpy\n",
        "  - pandas\n",
        "  - scipy\n",
        "  - seaborn\n",
        "```\n",
        "\n",
        "Le même fichier sous le format `requirements.txt` aura\n",
        "la forme suivante :\n",
        "\n",
        "``` python\n",
        "jupyter\n",
        "jupytext\n",
        "matplotlib\n",
        "nbconvert\n",
        "numpy\n",
        "pandas\n",
        "scipy\n",
        "seaborn\n",
        "```\n",
        "\n",
        "Sous leur apparente équivalence, au-delà de\n",
        "la question du formatage, ces fichiers ont\n",
        "deux différences principales :\n",
        "\n",
        "-   la version minimale de `Python` est définie dans\n",
        "    le fichier `environment.yml` alors qu’elle ne l’est\n",
        "    pas dans un fichier `requirements.txt`. C’est\n",
        "    parce que le second installe les dépendances dans\n",
        "    un environnement déjà existant par ailleurs alors\n",
        "    que le premier peut servir à créer l’environnement\n",
        "    avec une certaine configuration de `Python` ;\n",
        "-   le mode d’installation des *packages* n’est pas le\n",
        "    même. Avec un `environment.yml` on installera des\n",
        "    packages via conda alors qu’avec un `requirements.txt`\n",
        "    on privilégiera plutôt `pip`[2].\n",
        "\n",
        "Dans le cas de l’environnement `conda`,\n",
        "le choix du *channel* `conda-forge` vise à contrôler le dépôt utilisé par\n",
        "`Anaconda`.\n",
        "\n",
        "[1] Sur la différence entre les environnements virtuels\n",
        "et les environnements conda, voir\n",
        "[cette partie](https://ensae-reproductibilite.github.io/website/portability/#les-environnements-virtuels-) de cours\n",
        "plus avancé que nous donnons\n",
        "avec Romain Avouac sur la mise en production\n",
        "de projets *data science*.\n",
        "\n",
        "[2] Il est possible d’installer une partie des *packages*\n",
        "avec `pip` en définissant un champ `pip` dans le\n",
        "fichier `environment.yml`. Néanmoins, les concepteurs\n",
        "d’Anaconda recommandent d’être prudent avec cette méthode\n",
        "qui présente certes l’avantage d’accélérer le temps de\n",
        "création de l’environnement mais peut créer des\n",
        "difficultés avec des librairies nécessitant d’autres\n",
        "langages système comme le `C`."
      ],
      "id": "ac321c76-128a-49a5-8992-f8177f3f5c3d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-warning\" role=\"alert\">\n",
        "<h3 class=\"alert-heading\"><i class=\"fa-solid fa-lightbulb\"></i> Hint</h3>\n",
        "\n",
        "La `conda forge` est un dépôt de *package* alternatif\n",
        "au canal par défaut d’Anaconda qui est maintenu par\n",
        "l’équipe de développeurs officiels d’Anaconda.\n",
        "Comme cette dernière cherche en priorité à\n",
        "assurer la stabilité de l’écosystème `Anaconda`,\n",
        "les versions de *package* évoluent moins vite\n",
        "que le rythme voulu par les développeurs de\n",
        "*packages*. Pour cette raison, un dépôt\n",
        "alternatif, où les montées de version sont\n",
        "plus simples parce qu’elles dépendent des\n",
        "développeurs de chaque package, a émergé.\n",
        "Il s’agit de la `conda forge`. Lorsqu’on\n",
        "désire utiliser des fonctionalités récentes\n",
        "de l’écosystème de la *data science*,\n",
        "il est conseillé de l’utiliser.\n",
        "\n",
        "</div>"
      ],
      "id": "dcb988ec-5d69-46d7-a27c-c81e66705f99"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ne pas oublier de mettre ce fichier sous contrôle de version et de l’envoyer\n",
        "sur le dépôt par un `push`.\n",
        "\n",
        "### 3.2.2 Créer un environnement reproductible dans Github Actions\n",
        "\n",
        "Deux approches sont possibles à ce niveau, selon le degré\n",
        "de reproductibilité désiré[1]:\n",
        "\n",
        "-   Créer l’environnement via une action existante. L’action\n",
        "    [`conda-incubator/setup-miniconda@v2`](https://github.com/conda-incubator/setup-miniconda)\n",
        "    est un bon point de départ.\n",
        "-   Créer l’environnement dans une image `Docker`.\n",
        "\n",
        "La deuxième solution permet de contrôler de manière\n",
        "beaucoup plus fine l’environnement dans lequel\n",
        "`Python` s’éxécutera ainsi que la manière dont\n",
        "l’environnement sera créé[2]. Néanmoins, elle nécessite\n",
        "des connaissances plus poussées dans la principe\n",
        "de la conteneurisation qui peuvent être coûteuses\n",
        "à acquérir. Selon l’ambition du projet, notamment\n",
        "les réutilisation qu’il désire,\n",
        "un *data scientist* pourra privilégier\n",
        "telle ou telle option. Les deux solutions sont présentées\n",
        "dans l’exemple fil-rouge du cours que nous\n",
        "donnons avec Romain Avouac\n",
        "(https://ensae-reproductibilite.github.io/website/application/).\n",
        "\n",
        "### 3.2.3 Tester un notebook `myfile.ipynb`\n",
        "\n",
        "Dans cette partie, on va supposer que le *notebook* à tester s’appelle `myfile.ipynb`\n",
        "et se trouve à la racine du dépôt. Les\n",
        "dépendances pour l’exécuter sont\n",
        "listées dans un fichier `requirements.txt`.\n",
        "\n",
        "Le modèle suivant, expliqué en dessous, fournit un modèle de recette pour\n",
        "tester un notebook. Supposons que ce fichier soit présent\n",
        "dans un chemin `.github/workflows/test-notebook.yml`\n",
        "\n",
        "[1] Le point de vue que nous défendons avec\n",
        "Romain Avouac dans notre cours sur la reproductibilité\n",
        "est qu’il s’agit d’un *continuum* dans lequel on investit\n",
        "plus ou moins en fonction de ses contraintes, de ses\n",
        "besoins, de ses compétences, du temps humain qu’on\n",
        "peut dédier à développer des output reproductibles\n",
        "et le temps gagné en développant une telle approche.\n",
        "Selon où on se trouve sur ce cursus, en fonction\n",
        "des solutions déjà existantes qu’on peut trouver\n",
        "sur internet, on va plus ou moins raffiner\n",
        "notre intégration et nos déploiements\n",
        "continus.\n",
        "\n",
        "[2] Il est recommandé de ne pas garder la période de rétention\n",
        "des artefacts par défaut car celle-ci est assez longue (90 jours).\n",
        "Les *output* pouvant être assez volumineux et expirant rapidement\n",
        "(en général ce qui nous intéresse est la dernière ou l’avant\n",
        "dernière version de l’\\_output), pour des raisons écologiques,\n",
        "il est recommandé de fixer des périodes courtes. Cela peut être\n",
        "fait directement dans le fichier configurant l’intégration\n",
        "continue comme ici ou dans les paramètres par défaut\n",
        "du dépôt pour que cette règle s’applique à toutes les\n",
        "productions faites par intégration continue."
      ],
      "id": "f38ca6f5-a01f-4fd6-8082-d742b3a1a8e9"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<details>"
      ],
      "id": "38c69542-93a2-4de1-a5cc-6956fd35e438"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<summary>"
      ],
      "id": "a3c23dee-9cc2-4c4f-be72-3779a66e790f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Environnement virtuel"
      ],
      "id": "89f4b0eb-84c8-44fe-9935-b4b70839c511"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</summary>"
      ],
      "id": "7ad45236-4004-4796-b74d-a0749b1b6860"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``` yaml\n",
        "name: Test notebook execution using Github Actions\n",
        "\n",
        "on: [push]\n",
        "\n",
        "jobs:\n",
        "  build-linux:\n",
        "    runs-on: ubuntu-latest\n",
        "\n",
        "    steps:\n",
        "    - uses: actions/checkout@v3\n",
        "    - name: Set up Python 3.10\n",
        "      uses: actions/setup-python@v3\n",
        "      with:\n",
        "        python-version: '3.10'\n",
        "      - shell: bash\n",
        "      run: |\n",
        "        python --version\n",
        "    - name: Install dependencies\n",
        "      run:\n",
        "        pip install -r requirements.txt\n",
        "        pip install jupyter nbconvert\n",
        "    - name: Test jupyter from command line\n",
        "      run:\n",
        "        jupyter nbconvert --execute --to notebook --inplace myfile.ipynb\n",
        "    - uses: actions/upload-artifact@v3\n",
        "      with:\n",
        "        name: Notebook\n",
        "        path: myfile.ipynb\n",
        "        retention-days: 5\n",
        "```"
      ],
      "id": "92e42188-7890-471c-8b3f-3a565ec46f55"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</details>"
      ],
      "id": "be319b0a-a658-4d32-bb89-6c60107c643f"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<details>"
      ],
      "id": "617db3e4-a1e5-40f4-8da7-a4280e5e1ace"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<summary>"
      ],
      "id": "206bccdb-5449-4853-aa38-0641f5edc4c1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Environnement conda"
      ],
      "id": "4427dc94-7f61-4f4d-8cf2-f9ee008b5afd"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</summary>"
      ],
      "id": "3686e001-ea27-44b8-95d0-7bdc943ac529"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``` yaml\n",
        "name: Test notebook execution using Github Actions\n",
        "\n",
        "on: [push]\n",
        "\n",
        "jobs:\n",
        "  build-linux:\n",
        "    runs-on: ubuntu-latest\n",
        "\n",
        "    steps:\n",
        "    - uses: actions/checkout@v3\n",
        "    - name: Set up Python 3.10\n",
        "      uses: actions/setup-python@v3\n",
        "      with:\n",
        "        python-version: '3.10'\n",
        "    - name: Add conda to system path\n",
        "      run: |\n",
        "        # $CONDA is an environment variable pointing to the root of the miniconda directory\n",
        "        echo $CONDA/bin >> $GITHUB_PATH\n",
        "    - name: Install dependencies\n",
        "      run: |\n",
        "        conda env update --file environment.yml --name base\n",
        "        conda install jupyter nbconvert\n",
        "    - name: Test jupyter from command line\n",
        "      run:\n",
        "        jupyter nbconvert --execute --to notebook --inplace myfile.ipynb\n",
        "    - uses: actions/upload-artifact@v3\n",
        "      with:\n",
        "        name: Notebook\n",
        "        path: myfile.ipynb\n",
        "        retention-days: 5\n",
        "```"
      ],
      "id": "8bd51205-2a96-4d91-8cc3-7169583511c8"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</details>"
      ],
      "id": "3f02d7a5-3d11-49f0-add3-69cf45acb099"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dans les deux cas, la démarche est la même:\n",
        "\n",
        "-   on récupère les fichiers présents dans le dépôt\n",
        "    (action `checkout`) ;\n",
        "-   on installe `Python` ;\n",
        "-   on installe les dépendances pour exécuter le code.\n",
        "    Dans l’approche `conda`, il est également nécessaire\n",
        "    de faire quelques configurations supplémentaires (notamment\n",
        "    ajouter `conda` aux logiciels reconnus par la ligne\n",
        "    de commande) ;\n",
        "-   on teste le notebook en ligne de commande et remplace\n",
        "    celui existant, sur la machine temporaire, par la version\n",
        "    produite sur cet environnement neutre.\n",
        "-   on rend possible le téléchargement du\n",
        "    notebook produit automatiquement pendant 5 jours[1]. Ceci\n",
        "    repose sur les *artefacts* qui sont un élément récupéré\n",
        "    des machines temporaires qui n’existent plus dès que le\n",
        "    code a fini d’être exécuté.\n",
        "\n",
        "Ces actions sont exécutées à chaque interaction avec\n",
        "le dépôt distant (`push`), quelle que soit la\n",
        "branche. A partir de ce modèle, il est possible de\n",
        "raffiner pour, par exemple, automatiquement\n",
        "faire un commit du notebook validé et le pusher\n",
        "via le robot `Github`[2]\n",
        "\n",
        "# 4. Mettre à disposition en continu des valorisations du projet\n",
        "\n",
        "Les projets de valorisation de données prennent des formes\n",
        "très variées et s’adressent à des publics multiples dont\n",
        "les attentes peuvent être très diverses.\n",
        "Ne pas attendre la finalisation d’un projet pour mettre\n",
        "en oeuvre certains livrables est une méthode efficace\n",
        "pour ne pas se retrouver noyé, au dernier moment,\n",
        "sous des demandes et de nouvelles contraintes.\n",
        "\n",
        "La production en continu de livrables est donc une\n",
        "méthode très prisée dans le monde de la donnée.\n",
        "Les principaux fournisseurs de services\n",
        "d’intégration continue, à commencer par\n",
        "`Github` et `Gitlab` proposent des services\n",
        "pour le déploiement en continu. Cependant,\n",
        "ceux-ci ne sont adaptés qu’à certains types\n",
        "de livrables, principalement la mise à disposition\n",
        "de sites internet, et il peut être intéressant\n",
        "d’utiliser des services externes ou une\n",
        "infrastructures `Kubernetes` selon les\n",
        "moyens à dispositon et les besoins des utilisateurs.\n",
        "\n",
        "## 4.1 Les services de mise à disposition de `Github` et `Gitlab`\n",
        "\n",
        "`Github` et `Gitlab`, les deux plateformes de partage\n",
        "de code, proposent non seulement des services\n",
        "gratuits d’intégration continue mais aussi des services\n",
        "de mise à disposition de sites web pleinement intégrés\n",
        "aux services de stockage de code.\n",
        "\n",
        "Ces services, `Gitlab Pages` et `Github Pages`, auxquels\n",
        "on peut associer le service externe `Netlify` qui répond\n",
        "au même principe[3] permettent, à chaque modification\n",
        "du code source d’un projet, de reconstruire le site web (le livrable)\n",
        "qui peut être directement produit à partir de certains fichiers\n",
        "(des slides `revealJS` par exemple) ou qui\n",
        "sert d’output à l’intégration continue après compilation\n",
        "de fichiers plus complexes (des fichiers `quarto` par exemple).\n",
        "\n",
        "Chaque dépôt sur `Github` ou `Gitlab` peut ainsi être associé\n",
        "à un URL de déploiement disponible sur internet. A chaque\n",
        "`commit` sur le dépôt, le site *web* qui sert de livrable\n",
        "est ainsi mis à jour. La version déployée à partir de la\n",
        "branche principale peut ainsi être considérée\n",
        "comme la version de production alors que les branches\n",
        "secondaires peuvent servir d’espace bac à sable pour\n",
        "vérifier que des changements dans le code source\n",
        "ne mettent pas en péril le livrable. Cette méthode,\n",
        "qui sécurise la production d’un livrable sous forme\n",
        "de site *web*, est ainsi particulièrement appréciable.\n",
        "\n",
        "## 4.2 Les services externes disponibles sans infrastructure spéciale\n",
        "\n",
        "Pour fonctionner, l’intégration continue\n",
        "nécessite de mettre en oeuvre des environnements normalisés.\n",
        "Comme évoqué précédemment,\n",
        "la technologie sous-jacente est celle de la conteneurisation.\n",
        "Les images qui servent de point de départ au lancement\n",
        "d’un conteneur sont elles-mêmes mises à disposition\n",
        "dans des espaces communautaires (des registres d’images).\n",
        "Il en existe plusieurs, les plus connus étant\n",
        "le `dockerhub` ou le `registry` de `Gitlab`.\n",
        "Ces registres servent d’espaces de stockage pour des images,\n",
        "qui sont des objets volumineux (potentiellement plusieurs\n",
        "Gigas) mais aussi d’espace de mutualisation en permettant\n",
        "à d’autres de réutiliser une image prête à l’emploi ou,\n",
        "au contraire, à partir de\n",
        "laquelle on peut ajouter un certain nombre de couches\n",
        "pour obtenir l’environnement minimal\n",
        "de reproductibilité.\n",
        "Il est possible d’utiliser certaines actions `Github`\n",
        "prête à l’emploi pour constuire une image `Docker`\n",
        "à partir d’un fichier `Dockerfile`. Après avoir\n",
        "crée une connexion entre un compte sur la\n",
        "plateforme `Github` et l’autre sur `DockerHub`,\n",
        "une mise à disposition automatisée d’un livrable\n",
        "sous forme d’image `Docker` est ainsi possible.\n",
        "\n",
        "Une image `Docker` peut offrir une grande variété\n",
        "d’*output*. Elle peut servir uniquement à\n",
        "mettre à disposition un environnement de\n",
        "reproductibilité mais elle peut servir à mettre\n",
        "à disposition, pour les personnes maîtrisant\n",
        "`Docker`, des *output* plus raffinés. Par exemple,\n",
        "dans le cours que nous donnons à l’ENSAE, nous\n",
        "montrons comment `docker` peut servir à\n",
        "mettre à disposition à un utilisateur tiers\n",
        "une application minimaliste (construite avec `flask`)\n",
        "qu’il fera tourner\n",
        "sur son ordinateur.\n",
        "\n",
        "Si une image `Docker` peut être très utile pour la mise\n",
        "à disposition, elle nécessite pour sa réutilisation\n",
        "un niveau avancé d’expertise en programmation.\n",
        "Cela ne conviendra pas à tous les publics. Certains\n",
        "ne désireront que bénéficier d’une application interactive\n",
        "où ils pourrons visualiser certains résultats en fonction\n",
        "d’actions comme des filtres sur des sous-champs ou le choix\n",
        "de certaines plages de données. D’autres publics seront\n",
        "plutôt intéressé par la réutilisation d’un programme\n",
        "ou des résultats d’un modèle sous forme d’API mais n’auront\n",
        "pas l’infrastructure interne pour faire tourner le code\n",
        "d’origine ou une image `Docker`. C’est pour répondre à ces\n",
        "limites qu’il peut devenir intéressant, pour une équipe\n",
        "de *data science* de développer une architecture\n",
        "`kubernetes` interne, si l’organisation en a les moyens, ou\n",
        "de payer un fournisseur de service, comme AWS, qui permet\n",
        "cela.\n",
        "\n",
        "## 4.3 `Kubernetes`: le sommet de la pente du déploiement\n",
        "\n",
        "`Kubernetes` est une technologie qui pousse la logique\n",
        "de la conteneurisation à son paroxysme.\n",
        "Il s’agit d’un système open-source, développé\n",
        "par `Google`, permettant\n",
        "d’automatiser le déploiement, la mise à l’échelle\n",
        "et la gestion d’applications conteneurisées.\n",
        "Grâce à Kubernetes, une application, par exemple\n",
        "un site web proposant de la réactivité,\n",
        "peut être mise à disposition et reporter les calculs,\n",
        "lorsqu’ils sont nécessaires, sur\n",
        "un serveur. L’utilisation de `Kubernetes` dans\n",
        "un projet de *data science* permet ainsi\n",
        "d’anticiper à la fois l’interface d’une application\n",
        "valorisant un projet mais aussi le fonctionnement\n",
        "du *back-office*, par exemple en testant la capacité\n",
        "de charge de cette application. Une introduction\n",
        "à `Kubernetes` orienté donnée peut être trouvée dans\n",
        "le [cours dédié à la mise en production](https://ensae-reproductibilite.github.io/website/)\n",
        "que nous donnons avec Romain Avouac et dans ce\n",
        "[post de blog](https://towardsdatascience.com/from-jupyter-to-kubernetes-refactoring-and-deploying-notebooks-using-open-source-tools-19f99585e923) très bien fait.\n",
        "\n",
        "Dans les grandes organisations, où les rôles sont\n",
        "plus spécialisés que dans les petites structures,\n",
        "ce ne sont pas nécessairement les *data scientists*\n",
        "qui devront maîtriser `Kubernetes` mais plutôt\n",
        "les *data-architect* ou les *data-engineer*. Néanmoins,\n",
        "les *data scientists* devront être capable de\n",
        "dialoguer avec eux et mettre en oeuvre une méthode\n",
        "de travail adaptée (celle-ci reposera en principe sur\n",
        "l’approche CI/CD). Dans les petites structures, les\n",
        "*data scientist* peuvent être en mesure\n",
        "de mettre en oeuvre le déploiement en continu. En\n",
        "revanche, il est plus rare, dans ces structures,\n",
        "où les moyens humains de maintenance sont limités,\n",
        "que les serveurs sur lesquels fonctionnent `Kubernetes`\n",
        "soient détenus en propres. En général, ils sont loués\n",
        "dans des services de paiement à la demande de type\n",
        "AWS.\n",
        "\n",
        "# 5. Références\n",
        "\n",
        "-   https://ensae-reproductibilite.github.io/website/\n",
        "-   https://towardsdatascience.com/from-jupyter-to-kubernetes-refactoring-and-deploying-notebooks-using-open-source-tools-19f99585e923\n",
        "\n",
        "[1] Il est recommandé de ne pas garder la période de rétention\n",
        "des artefacts par défaut car celle-ci est assez longue (90 jours).\n",
        "Les *output* pouvant être assez volumineux et expirant rapidement\n",
        "(en général ce qui nous intéresse est la dernière ou l’avant\n",
        "dernière version de l’\\_output), pour des raisons écologiques,\n",
        "il est recommandé de fixer des périodes courtes. Cela peut être\n",
        "fait directement dans le fichier configurant l’intégration\n",
        "continue comme ici ou dans les paramètres par défaut\n",
        "du dépôt pour que cette règle s’applique à toutes les\n",
        "productions faites par intégration continue.\n",
        "\n",
        "[2] Il s’agit du service utilisé, par exemple,\n",
        "pour ce cours. `Netlify` est un service de mise à disposition\n",
        "qui offre des fonctionalités plus complètes que celles\n",
        "permises par `Gitlab Pages` et `Github Pages`. Outre cet\n",
        "avantage, il est plus facile à configurer que `Github Pages`\n",
        "qui nécessite l’usage d’une branche dédiée nommée `gh-pages`,\n",
        "ce qui peut\n",
        "rebutant.\n",
        "\n",
        "[3] Il s’agit du service utilisé, par exemple,\n",
        "pour ce cours. `Netlify` est un service de mise à disposition\n",
        "qui offre des fonctionalités plus complètes que celles\n",
        "permises par `Gitlab Pages` et `Github Pages`. Outre cet\n",
        "avantage, il est plus facile à configurer que `Github Pages`\n",
        "qui nécessite l’usage d’une branche dédiée nommée `gh-pages`,\n",
        "ce qui peut\n",
        "rebutant."
      ],
      "id": "1b409c85-8714-4371-bb78-f06d3714d9f3"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "/opt/conda/share/jupyter/kernels/python3"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  }
}