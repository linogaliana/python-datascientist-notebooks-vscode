{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Premier pas vers l’industrialisation avec les pipelines scikit\n",
        "\n",
        "Lino Galiana  \n",
        "2025-03-19\n",
        "\n",
        "<div class=\"badge-container\"><div class=\"badge-text\">Pour essayer les exemples présents dans ce tutoriel :</div><a href=\"https://github.com/linogaliana/python-datascientist-notebooks/blob/main/notebooks/modelisation/6_pipeline.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/static/v1?logo=github&label=&message=View%20on%20GitHub&color=181717\" alt=\"View on GitHub\"></a>\n",
        "<a href=\"https://datalab.sspcloud.fr/launcher/ide/vscode-python?autoLaunch=true&name=«6_pipeline»&init.personalInit=«https%3A%2F%2Fraw.githubusercontent.com%2Flinogaliana%2Fpython-datascientist%2Fmain%2Fsspcloud%2Finit-vscode.sh»&init.personalInitArgs=«modelisation%206_pipeline%20correction»\" target=\"_blank\" rel=\"noopener\"><img src=\"https://custom-icon-badges.demolab.com/badge/SSP%20Cloud-Lancer_avec_VSCode-blue?logo=vsc&logoColor=white\" alt=\"Onyxia\"></a>\n",
        "<a href=\"https://datalab.sspcloud.fr/launcher/ide/jupyter-python?autoLaunch=true&name=«6_pipeline»&init.personalInit=«https%3A%2F%2Fraw.githubusercontent.com%2Flinogaliana%2Fpython-datascientist%2Fmain%2Fsspcloud%2Finit-jupyter.sh»&init.personalInitArgs=«modelisation%206_pipeline%20correction»\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/SSP%20Cloud-Lancer_avec_Jupyter-orange?logo=Jupyter&logoColor=orange\" alt=\"Onyxia\"></a>\n",
        "<a href=\"https://colab.research.google.com/github/linogaliana/python-datascientist-notebooks-colab//blob/main//notebooks/modelisation/6_pipeline.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a><br></div>\n",
        "\n",
        "Ce chapitre présente la première application\n",
        "d’une journée de cours que j’ai\n",
        "donné à l’Université Dauphine dans le cadre\n",
        "des *PSL Data Week*."
      ],
      "id": "15a07c93-f35f-4b0f-98fa-32ad795c111b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "<summary>\n",
        "\n",
        "Dérouler les _slides_ associées ci-dessous ou [cliquer ici](https://linogaliana.github.io/dauphine-week-data/#/title-slide)\n",
        "pour les afficher en plein écran.\n",
        "\n",
        "</summary>\n",
        "\n",
        "\n",
        "<div class=\"sourceCode\" id=\"cb1\"><pre class=\"sourceCode yaml code-with-copy\"><code class=\"sourceCode yaml\"></code><button title=\"Copy to Clipboard\" class=\"code-copy-button\"><i class=\"bi\"></i></button></pre><iframe class=\"sourceCode yaml code-with-copy\" src=\"https://linogaliana.github.io/dauphine-week-data/#/title-slide\"></iframe></div>\n",
        "\n",
        "</details>"
      ],
      "id": "420d8c99-ca67-4942-91af-fcc8502d8ea5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pour lire les données de manière efficace, nous\n",
        "proposons d’utiliser le *package* `duckdb`.\n",
        "Pour l’installer, voici la commande :"
      ],
      "id": "e65ad147-d4d9-43ee-a768-16d7c50f8035"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install duckdb"
      ],
      "id": "b16583c2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Pourquoi utiliser les *pipelines* ?\n",
        "\n",
        "## 1.1 Définitions préalables\n",
        "\n",
        "Ce chapitre nous amènera à explorer plusieurs écosystèmes, pour lesquels on retrouve quelques buzz-words dont voici les définitions :\n",
        "\n",
        "| Terme | Définition |\n",
        "|---------------------------|---------------------------------------------|\n",
        "| *DevOps* | Mouvement en ingénierie informatique et une pratique technique visant à l’unification du développement logiciel (dev) et de l’administration des infrastructures informatiques (ops) |\n",
        "| *MLOps* | Ensemble de pratiques qui vise à déployer et maintenir des modèles de machine learning en production de manière fiable et efficace |\n",
        "\n",
        "Ce chapitre fera des références régulières au cours\n",
        "de 3e année de l’ENSAE\n",
        "[*“Mise en production de projets data science”*](https://ensae-reproductibilite.github.io/website/).\n",
        "\n",
        "## 1.2 Objectif\n",
        "\n",
        "Les chapitres précédents ont permis de montrer des bouts de code\n",
        "épars pour entraîner des modèles ou faire du *preprocessing*.\n",
        "Cette démarche est intéressante pour tâtonner mais risque d’être coûteuse\n",
        "ultérieurement s’il est nécessaire d’ajouter une étape de *preprocessing*\n",
        "ou de changer d’algorithme.\n",
        "\n",
        "Les *pipelines* sont pensés pour simplifier la mise en production\n",
        "ultérieure d’un modèle de *machine learning*.\n",
        "Ils sont au coeur de la démarche de *MLOps* qui est\n",
        "présentée\n",
        "dans le cours de 3e année de l’ENSAE\n",
        "de [*“Mise en production de projets data science”*](https://ensae-reproductibilite.github.io/website/),\n",
        "qui vise à simplifier la mise en oeuvre opérationnelle de\n",
        "projets utilisant des techniques de *machine learning*."
      ],
      "id": "79b7b5f3-67d5-4cb6-8e55-3722bcbaf986"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "id": "23c02fb4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.3 Les *pipelines* `Scikit`\n",
        "\n",
        "Heureusement, `Scikit` propose un excellent outil pour proposer un cadre\n",
        "général pour créer une chaîne de production *machine learning*. Il\n",
        "s’agit des\n",
        "[*pipelines*](https://scikit-learn.org/stable/modules/compose.html).\n",
        "Ils présentent de nombreux intérêts, parmi lesquels :\n",
        "\n",
        "-   Ils sont très **pratiques** et **lisibles**. On rentre des données en entrée, on n’appelle qu’une seule fois les méthodes `fit` et `predict` ce qui permet de s’assurer une gestion cohérente des transformations de variables, par exemple après l’appel d’un `StandardScaler` ;\n",
        "-   La **modularité** rend aisée la mise à jour d’un pipeline et renforce la capacité à le réutiliser ;\n",
        "-   Ils permettent de facilement chercher les hyperparamètres d’un modèle. Sans *pipeline*, écrire un code qui fait du *tuning* d’hyperparamètres peut être pénible. Avec les *pipelines*, c’est une ligne de code ;\n",
        "-   La **sécurité** d’être certain que les étapes de preprocessing sont bien appliquées aux jeux de données désirés avant l’estimation.\n",
        "\n",
        "<div class=\"alert alert-success\" role=\"alert\">\n",
        "<h3 class=\"alert-heading\"><i class=\"fa-solid fa-lightbulb\"></i> Tip</h3>\n",
        "\n",
        "Un des intérêts des *pipelines* scikit est qu’ils fonctionnent aussi avec\n",
        "des méthodes qui ne sont pas issues de `scikit`.\n",
        "\n",
        "Il est possible d’introduire un modèle de réseau de neurone `Keras` dans\n",
        "un pipeline `scikit`.\n",
        "Pour introduire un modèle économétrique `statsmodels`\n",
        "c’est un peu plus coûteux mais nous allons proposer des exemples\n",
        "qui peuvent servir de modèle et qui montrent que c’est faisable\n",
        "sans trop de difficulté.\n",
        "\n",
        "</div>\n",
        "\n",
        "# 2. Comment créer un *pipeline*\n",
        "\n",
        "Un *pipeline* est un enchaînement d’opérations qu’on code en enchainant\n",
        "des pairs *(clé, valeur)* :\n",
        "\n",
        "-   la clé est le nom du pipeline, cela peut être utile lorsqu’on va\n",
        "    représenter le *pipeline* sous forme de diagramme acyclique (visualisation DAG)\n",
        "    ou qu’on veut afficher des informations sur une étape\n",
        "-   la valeur représente la transformation à mettre en oeuvre dans le *pipeline*\n",
        "    (c’est-à-dire, à l’exception de la dernière étape,\n",
        "    mettre en oeuvre une méthode `transform` et éventuellement une\n",
        "    transformation inverse)."
      ],
      "id": "c92f2df6-1214-48b2-a9c2-b00f1d9211be"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "estimators = [('reduce_dim', PCA()), ('clf', SVC())]\n",
        "pipe = Pipeline(estimators)\n",
        "pipe"
      ],
      "id": "40cbcbd1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Au sein d’une étape de *pipeline*, les paramètres d’un estimateur\n",
        "sont accessibles avec la notation `<estimator>__<parameter>`.\n",
        "Cela permet de fixer des valeurs pour les arguments des fonctions `scikit`\n",
        "qui sont appelées au sein d’un *pipeline*.\n",
        "C’est cela qui rendra l’approche des pipelines particulièrement utile\n",
        "pour la *grid search* :"
      ],
      "id": "5ab4e9c4-1a6d-4435-85cb-dc714ad8273e"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "param_grid = {\"reduce_dim__n_components\":[2, 5, 10], \"clf__C\":[0.1, 10, 100]}\n",
        "grid_search = GridSearchCV(pipe, param_grid=param_grid)\n",
        "grid_search"
      ],
      "id": "d284444c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ces *pipelines* sont initialisés sans données, il s’agit d’une structure formelle\n",
        "que nous allons ensuite ajuster en entraînant des modèles.\n",
        "\n",
        "## 2.1 Données utilisées\n",
        "\n",
        "Nous allons utiliser les données\n",
        "de transactions immobilières [DVF](https://app.dvf.etalab.gouv.fr/) pour chercher\n",
        "la meilleure manière de prédire, sachant les caractéristiques d’un bien, son\n",
        "prix.\n",
        "\n",
        "Ces données sont mises à disposition\n",
        "sur [`data.gouv`](https://www.data.gouv.fr/fr/datasets/demandes-de-valeurs-foncieres/).\n",
        "Néanmoins, le format csv n’étant pas pratique pour importer des jeux de données\n",
        "volumineux, nous proposons de privilégier la version `Parquet` mise à\n",
        "disposition par Eric Mauvière sur [`data.gouv`](https://www.data.gouv.fr/fr/datasets/dvf-2022-format-parquet/#/discussions).\n",
        "L’approche la plus efficace pour lire ces données est\n",
        "d’utiliser `DuckDB` afin de lire le fichier, extraire les colonnes\n",
        "d’intérêt puis passer à `Pandas` (pour en savoir plus sur\n",
        "l’intérêt de `DuckDB` pour lire des fichiers volumineux, vous pouvez\n",
        "consulter [ce post de blog](https://ssphub.netlify.app/post/parquetrp/) ou\n",
        "[celui-ci](https://www.icem7.fr/outils/3-explorations-bluffantes-avec-duckdb-1-interroger-des-fichiers-distants/) écrit\n",
        "par Eric Mauvière).\n",
        "\n",
        "Même si, en soi, les gains de temps sont faibles car `DuckDB` optimise\n",
        "les requêtes HTTPS nécessaires à l’import des données, nous proposons\n",
        "de télécharger les données pour réduire les besoins de bande passante."
      ],
      "id": "3134ba5e-da17-4a50-b0e3-8a62fe322425"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import os\n",
        "\n",
        "url = \"https://www.data.gouv.fr/fr/datasets/r/56bde1e9-e214-408b-888d-34c57ff005c4\"\n",
        "file_name = \"dvf.parquet\"\n",
        "\n",
        "# Check if the file already exists\n",
        "if not os.path.exists(file_name):\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        with open(file_name, \"wb\") as f:\n",
        "            f.write(response.content)\n",
        "        print(\"Téléchargement réussi.\")\n",
        "    else:\n",
        "        print(f\"Échec du téléchargement. Code d'état : {response.status_code}\")\n",
        "else:\n",
        "    print(f\"Le fichier '{file_name}' existe déjà. Aucun téléchargement nécessaire.\")"
      ],
      "id": "9ad0baa1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En premier lieu, puisque cela va faciliter les requêtes SQL ultérieures, on crée\n",
        "une vue :"
      ],
      "id": "a25b75cd-1175-4273-a2df-7fcb805b1c25"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import duckdb\n",
        "duckdb.sql(f'CREATE OR REPLACE VIEW dvf AS SELECT * FROM read_parquet(\"dvf.parquet\")')"
      ],
      "id": "5a9fbeca"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Les données prennent la forme suivante :"
      ],
      "id": "c85b8f5a-6c3b-44a4-805d-0b911ae0b48e"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "duckdb.sql(f\"SELECT * FROM dvf LIMIT 5\")"
      ],
      "id": "cf445e25"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Les variables que nous allons conserver sont les suivantes,\n",
        "nous allons les reformater pour la suite de l’exercice."
      ],
      "id": "fe0929c0-3777-4617-9303-5915e2fb82a6"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "xvars = [\n",
        "    \"Date mutation\", \"Valeur fonciere\",\n",
        "    'Nombre de lots', 'Code type local',\n",
        "    'Nombre pieces principales'\n",
        "]\n",
        "xvars = \", \".join([f'\"{s}\"' for s in xvars])"
      ],
      "id": "0d4e2158"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "mutations = duckdb.sql(\n",
        "    f'''\n",
        "    SELECT\n",
        "    date_part('month', \"Date mutation\") AS month,\n",
        "    substring(\"Code postal\", 1, 2) AS dep,\n",
        "    {xvars},\n",
        "    COLUMNS('Surface Carrez.*')\n",
        "    FROM dvf\n",
        "    '''\n",
        ").to_df()\n",
        "\n",
        "colonnes_surface = mutations.columns[mutations.columns.str.startswith('Surface Carrez')]\n",
        "mutations.loc[:, colonnes_surface] = mutations.loc[:, colonnes_surface].replace({',': '.'}, regex=True).astype(float).fillna(0)"
      ],
      "id": "74f9f841"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\" role=\"alert\">\n",
        "<h3 class=\"alert-heading\"><i class=\"fa-solid fa-comment\"></i> Note</h3>\n",
        "\n",
        "Le fichier `Parquet` mis à disposition sur `data.gouv` présente une incohérence de mise en forme de\n",
        "certaines colonnes à cause des virgules qui empêchent le formattage sous forme de colonne\n",
        "numérique.\n",
        "\n",
        "Le code ci-dessus effectue la conversion adéquate au niveau de `Pandas`.\n",
        "\n",
        "</div>"
      ],
      "id": "76f103fa-3c63-445f-b61b-056272e97053"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "mutations.head(2)"
      ],
      "id": "35a64220"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "\n",
        "<summary>\n",
        "\n",
        "Introduire un effet confinement\n",
        "\n",
        "</summary>\n",
        "\n",
        "Si vous travaillez avec les données de 2020, n’oubliez pas\n",
        "d’intégrer l’effet\n",
        "confinement dans vos modèles puisque cela a lourdement\n",
        "affecté les possibilités de transaction sur cette période, donc\n",
        "l’effet potentiel de certaines variables explicatives du prix.\n",
        "\n",
        "Pour introduire cet effet, vous pouvez créer une variable\n",
        "indicatrice entre les dates en question:\n",
        "\n",
        "``` python\n",
        "mutations['confinement'] = (\n",
        "    mutations['Date mutation']\n",
        "    .between(pd.to_datetime(\"2020-03-17\"), pd.to_datetime(\"2020-05-03\"))\n",
        "    .astype(int)\n",
        ")\n",
        "```\n",
        "\n",
        "Comme nous travaillons sur les données de 2022,\n",
        "nous pouvons nous passer de cette variable.\n",
        "\n",
        "</details>"
      ],
      "id": "df88e0f5-7b7c-4638-bb66-de8862a3ea55"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Les données DVF proposent une observation par transaction.\n",
        "Ces transactions\n",
        "peuvent concerner plusieurs lots. Par exemple, un appartement\n",
        "avec garage et cave comportera trois lots.\n",
        "\n",
        "Pour simplifier,\n",
        "on va créer une variable de surface qui agrège les différentes informations\n",
        "de surface disponibles dans le jeu de données.\n",
        "Les agréger revient à supposer que le modèle de fixation des prix est le même\n",
        "entre chaque lot. C’est une hypothèse simplificatrice qu’une personne plus\n",
        "experte du marché immobilier, ou qu’une approche propre de sélection\n",
        "de variable pourrait amener à nier. En effet, les variables\n",
        "en question sont faiblement corrélées les unes entre elles, à quelques\n",
        "exceptions près (<a href=\"#fig-corr-surface\" class=\"quarto-xref\">Figure 2.1</a>):"
      ],
      "id": "fe38fbdd-ec6e-4298-8a54-d82491f1c36d"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "corr = mutations.loc[\n",
        "    :,\n",
        "    mutations.columns[mutations.columns.str.startswith('Surface Carrez')].tolist()\n",
        "]\n",
        "corr.columns = corr.columns.str.replace(\"Carrez du \", \"\")\n",
        "corr = corr.corr()\n",
        "\n",
        "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "\n",
        "cmap = sns.diverging_palette(230, 20, as_cmap=True)"
      ],
      "id": "3b539dfe"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``` python\n",
        "fig, ax = plt.subplots(1)\n",
        "g = sns.heatmap(\n",
        "    corr, ax=ax, \n",
        "    mask=mask,\n",
        "    vmax=.3, center=0,\n",
        "    square=True, linewidths=.5, cbar_kws={\"shrink\": .5},\n",
        "    xticklabels=corr.columns.values,\n",
        "    yticklabels=corr.columns.values, cmap=cmap, annot=True, fmt=\".2f\"\n",
        ")\n",
        "g\n",
        "```\n",
        "\n",
        "Figure 2.1"
      ],
      "id": "a2a8c193-31c1-4449-a7b1-3c09f747c121"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "mutations['lprix'] = np.log(mutations[\"Valeur fonciere\"])\n",
        "mutations['surface'] = mutations.loc[:, colonnes_surface].sum(axis = 1).astype(int)"
      ],
      "id": "0d6ab1b2"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "mutations['surface'] = mutations.loc[:, mutations.columns[mutations.columns.str.startswith('Surface Carrez')].tolist()].sum(axis = 1)"
      ],
      "id": "48f703fb"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Un premier pipeline : *random forest* sur des variables standardisées\n",
        "\n",
        "Notre premier *pipeline* va nous permettre d’intégrer ensemble:\n",
        "\n",
        "1.  Une étape de *preprocessing* avec la standardisation de variables\n",
        "2.  Une étape d’estimation du prix en utilisant un modèle de *random forest*\n",
        "\n",
        "Pour le moment, on va prendre comme acquis un certain nombre de variables\n",
        "explicatives (les *features*) et les hyperparamètres du modèle.\n",
        "\n",
        "L’algorithme des *random forest* est une technique statistique basée sur\n",
        "les arbres de décision. Elle a été définie explicitement par l’un\n",
        "des pionniers du *machine learning*, Breiman (2001).\n",
        "Il s’agit d’une [méthode ensembliste](https://en.wikipedia.org/wiki/Ensemble_learning)\n",
        "puisqu’elle consiste à utiliser plusieurs algorithmes (en l’occurrence des arbres\n",
        "de décision) pour obtenir une meilleure prédiction que ne le permettraient\n",
        "chaque modèle isolément.\n",
        "\n",
        "Les *random forest* sont une méthode d’aggrégation[1] d’arbres de décision.\n",
        "On calcule $K$ arbres de décision et en tire, par une méthode d’agrégation,\n",
        "une règle de décision moyenne qu’on va appliquer pour tirer une\n",
        "prédiction de nos données.\n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*jE1Cb1Dc_p9WEOPMkC95WQ.png)\n",
        "\n",
        "L’un des intérêts\n",
        "des *random forest* est qu’il existe des méthodes pour déterminer\n",
        "l’importance relative de chaque variable dans la prédiction.\n",
        "\n",
        "Nous allons ici partir d’un *random forest* avec des valeurs d’hyperparamètres\n",
        "données, à savoir la profondeur de l’arbre.\n",
        "\n",
        "## 3.1 Définition des ensembles *train* et *test*\n",
        "\n",
        "Nous allons donc nous restreindre à un sous-ensemble de colonnes dans un\n",
        "premier temps.\n",
        "\n",
        "Nous allons également ne conserver que les\n",
        "transactions inférieures à 5 millions\n",
        "d’euros (on anticipe que celles ayant un montant supérieur sont des transactions\n",
        "exceptionnelles dont le mécanisme de fixation du prix diffère)\n",
        "\n",
        "[1] Les *random forest* sont l’une des principales méthodes\n",
        "ensemblistes. Outre cette approche, les plus connues sont\n",
        "le [*bagging* (*boostrap aggregating*)](https://en.wikipedia.org/wiki/Bootstrap_aggregating) et le *boosting*\n",
        "qui consistent à choisir la prédiction à privilégier\n",
        "selon des algorithmes de choix différens.\n",
        "Par exemple le *bagging* est une technique basée sur le vote majoritaire (Breiman 1996).\n",
        "Cette technique s’inspire du *bootstrap* qui, en économétrie,\n",
        "consiste à ré-estimer sur *K* sous-échantillons\n",
        "aléatoires des données un estimateur afin d’en tirer, par exemple, un intervalle\n",
        "de confiance empirique à 95%. Le principe du *bagging* est le même. On ré-estime\n",
        "*K* fois notre estimateur (par exemple un arbre de décision) et propose une\n",
        "règle d’agrégation pour en tirer une règle moyennisée et donc une prédiction.\n",
        "Le *boosting* fonctionne selon un principe différent, basé sur\n",
        "l’optimisation de combinaisons de classifieurs faibles."
      ],
      "id": "4fd112f1-9351-47b9-93f6-bb28983efce5"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "mutations2 = mutations.drop(\n",
        "    colonnes_surface.tolist() + [\"Date mutation\", \"lprix\"], # ajouter \"confinement\" si données 2020\n",
        "    axis = \"columns\"\n",
        "    ).copy()\n",
        "\n",
        "mutations2 = mutations2.loc[mutations2['Valeur fonciere'] < 5e6] #keep only values below 5 millions\n",
        "\n",
        "mutations2.columns = mutations2.columns.str.replace(\" \", \"_\")\n",
        "mutations2  = mutations2.dropna(subset = ['dep','Code_type_local','month'])"
      ],
      "id": "d53418cb"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notre *pipeline* va incorporer deux types de variables: les variables\n",
        "catégorielles et les variables numériques.\n",
        "Ces différents types vont bénéficier d’étapes de *preprocessing*\n",
        "différentes."
      ],
      "id": "9cf4d0d8-1193-464b-8163-bf60bf0ee351"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "numeric_features = mutations2.columns[~mutations2.columns.isin(['dep','Code_type_local', 'month', 'Valeur_fonciere'])].tolist()\n",
        "categorical_features = ['dep','Code_type_local','month']"
      ],
      "id": "c50eb3b3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Au passage, nous avons abandonné la variable de code postal pour privilégier\n",
        "le département afin de réduire la dimension de notre jeu de données. Si on voulait\n",
        "vraiment avoir un bon modèle, il faudrait faire autrement car le code postal\n",
        "est probablement un très bon prédicteur du prix d’un bien, une fois que\n",
        "les caractéristiques du bien sont contrôlées.\n",
        "\n",
        "<div class=\"alert alert-success\" role=\"alert\">\n",
        "<h3 class=\"alert-heading\"><i class=\"fa-solid fa-pencil\"></i> Exercice 1 : Découpage des échantillons</h3>\n",
        "\n",
        "Nous allons stratifier notre échantillonage de *train/test* par département\n",
        "afin de tenir compte, de manière minimale, de la géographie.\n",
        "Pour accélérer les calculs pour ce tutoriel, nous n’allons considérer que\n",
        "30% des transactions observées sur chaque département.\n",
        "\n",
        "Voici le code pour le faire:\n",
        "\n",
        "``` python\n",
        "mutations2 = mutations2.groupby('dep').sample(frac = 0.1, random_state = 123)\n",
        "```\n",
        "\n",
        "Avec la fonction adéquate de `Scikit`, faire un découpage de `mutations2`\n",
        "en *train* et *test sets*\n",
        "en suivant les consignes suivantes:\n",
        "\n",
        "-   20% des données dans l’échantillon de *test* ;\n",
        "-   L’échantillonnage est stratifié par départements ;\n",
        "-   Pour avoir des résultats reproductibles, choisir une racine égale à 123.\n",
        "\n",
        "</div>"
      ],
      "id": "827eb3cf-9cae-4216-a90a-78621f68e9f3"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "mutations2 = mutations2.groupby('dep').sample(frac = 0.1, random_state = 123)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    mutations2.drop(\"Valeur_fonciere\", axis = 1),\n",
        "    mutations2[[\"Valeur_fonciere\"]].values.ravel(),\n",
        "    test_size = 0.2, random_state = 123, stratify=mutations2[['dep']]\n",
        ")"
      ],
      "id": "a29c3e7b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2 Définition du premier *pipeline*\n",
        "\n",
        "Pour commencer, nous allons fixer la taille des arbres de décision avec\n",
        "l’hyperparamètre `max_depth = 2`.\n",
        "\n",
        "Notre *pipeline* va intégrer les étapes suivantes :\n",
        "\n",
        "1.  **Preprocessing** :\n",
        "    -   Les variables numériques vont être standardisées avec un `StandardScaler`.\n",
        "        Pour cela, nous allons utiliser la liste `numeric_features` définie précédemment.\n",
        "    -   Les variables catégorielles vont être explosées avec un *one hot encoding*\n",
        "        (méthode `OneHotEncoder` de `scikit`)\n",
        "        Pour cela, nous allons utiliser la liste `categorical_features`\n",
        "2.  **Random forest** : nous allons appliquer l’estimateur *ad hoc* de `Scikit`.\n",
        "\n",
        "<div class=\"alert alert-success\" role=\"alert\">\n",
        "<h3 class=\"alert-heading\"><i class=\"fa-solid fa-pencil\"></i> Exercice 2 : Construction d’un premier pipeline formel</h3>\n",
        "\n",
        "1.  Initialiser un *random forest* de profondeur 2. Fixer la racine à 123 pour avoir des résultats reproductibles.\n",
        "2.  La première étape du *pipeline* (nommer cette couche *preprocessor*) consiste à appliquer les étapes de *preprocessing* adaptées à chaque type de variables:\n",
        "    -   Pour les variables numériques, appliquer une étape d’imputation à la moyenne puis standardiser celles-ci\n",
        "    -   Pour les variables catégorielles, appliquer un [*one hot encoding*](https://en.wikipedia.org/wiki/One-hot)\n",
        "3.  Appliquer comme couche de sortie le modèle défini plus tôt.\n",
        "\n",
        "*💡 Il est recommandé de s’aider de la documentation de `Scikit`. Si vous avez besoin d’un indice supplémentaire, consulter le pipeline présenté ci-dessous.*\n",
        "\n",
        "</div>"
      ],
      "id": "cf72a17e-6f92-4d26-9d2d-394018d57794"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Question 1\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "regr = RandomForestRegressor(max_depth=2, random_state=123)"
      ],
      "id": "exo2-q1"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.pipeline import make_pipeline, Pipeline\n",
        "from sklearn.compose import make_column_transformer\n",
        "\n",
        "numeric_pipeline = make_pipeline(\n",
        "  SimpleImputer(),\n",
        "  StandardScaler()\n",
        ")\n",
        "transformer = make_column_transformer(\n",
        "    (numeric_pipeline, numeric_features),\n",
        "    (OneHotEncoder(sparse_output = False, handle_unknown = \"ignore\"), categorical_features))\n",
        "pipe = Pipeline(steps=[('preprocessor', transformer),\n",
        "                      ('randomforest', regr)])"
      ],
      "id": "6db909cf"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A l’issue de cet exercice, nous devrions obtenir le *pipeline* suivant."
      ],
      "id": "b993f5e1-d65e-4e7a-8979-887a2a443e62"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipe"
      ],
      "id": "9fe0764a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nous avons construit ce pipeline sous forme de couches successives. La couche\n",
        "`randomforest` prendra automatiquement le résultat de la couche `preprocessor`\n",
        "en *input*. La couche `features` permet d’introduire de manière relativement\n",
        "simple (quand on a les bonnes méthodes) la complexité du *preprocessing*\n",
        "sur données réelles dont les types divergent.\n",
        "\n",
        "A cette étape, rien n’a encore été estimé.\n",
        "C’est très simple à mettre en oeuvre\n",
        "avec un *pipeline*.\n",
        "\n",
        "<div class=\"alert alert-success\" role=\"alert\">\n",
        "<h3 class=\"alert-heading\"><i class=\"fa-solid fa-pencil\"></i> Exercice 3 : Mise en oeuvre du pipeline</h3>\n",
        "\n",
        "1.  Estimer les paramètres du modèle sur le jeu d’entraînement\n",
        "2.  Observer la manière dont les données d’entraînement sont transformées\n",
        "    par l’étape de *preprocessing* avec les méthodes adéquates sur 4 observations de `X_train`\n",
        "    tirées aléatoirement\n",
        "3.  Utiliser ce modèle pour prédire le prix sur l’échantillon de test. A partir de ces quelques prédictions,\n",
        "    quel semble être le problème ?\n",
        "4.  Observer la manière dont ce *preprocessing* peut s’appliquer sur deux exemples fictifs :\n",
        "    -   Un appartement (`code_type_local = 2`) dans le 75, vendu au mois de mai, unique lot de la vente avec 3 pièces, faisant 75m² ;\n",
        "    -   Une maison (`code_type_local = 1`) dans le 06, vendue en décembre, dans une transaction avec 2 lots. La surface complète est de 180m² et le bien comporte 6 pièces.\n",
        "5.  Déduire sur ces deux exemples le prix prédit par le modèle.\n",
        "6.  Calculer et interpréter le RMSE sur l’échantillon de test. Ce modèle est-il satisfaisant ?\n",
        "\n",
        "</div>"
      ],
      "id": "5e9e776c-68e6-4c50-9557-4dd6a8101dfe"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipe.fit(X_train, y_train)"
      ],
      "id": "c39961d1"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Question 2\n",
        "pipe[:-1].transform(X_train.sample(4))"
      ],
      "id": "2b3748da"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Question 4\n",
        "pipe.predict(X_test)"
      ],
      "id": "11f6ae3e"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Question 5\n",
        "X_fictif = pd.DataFrame(\n",
        "    {\n",
        "        \"month\": [3, 12],\n",
        "        \"dep\": [\"75\", \"06\"],\n",
        "        \"Nombre_de_lots\": [1, 2],\n",
        "        \"Code_type_local\": [2, 1],\n",
        "        \"Nombre_pieces_principales\": [3., 6.],\n",
        "        \"surface\": [75., 180.]\n",
        "    }\n",
        ")\n",
        "pipe[:-1].transform(X_fictif)\n",
        "pipe.predict(X_fictif)"
      ],
      "id": "96570892"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "np.sqrt(\n",
        "    mean_squared_error(\n",
        "        pipe.predict(X_test),\n",
        "        y_test\n",
        "    )\n",
        ")"
      ],
      "id": "cbc733c2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.3 *Variable importance*\n",
        "\n",
        "Les prédictions semblent avoir une assez faible variance, comme si des variables\n",
        "de seuils intervenaient. Nous allons donc devoir essayer de comprendre pourquoi.\n",
        "\n",
        "La *“variable importance”*\n",
        "se réfère à la mesure de l’influence de chaque variable d’entrée sur la performance du modèle.\n",
        "L’impureté fait référence à l’incertitude ou à l’entropie présente dans un ensemble de données.\n",
        "Dans le contexte des *random forest*, cette mesure est souvent calculée en évaluant la réduction moyenne de l’impureté des nœuds de décision causée par une variable spécifique.\n",
        "Cette approche permet de quantifier l’importance des variables dans le processus de prise de décision du modèle, offrant ainsi des intuitions sur les caractéristiques les plus informatives pour la prédiction (plus de détails [sur ce blog](https://mljar.com/blog/feature-importance-in-random-forest/)).\n",
        "\n",
        "On ne va représenter, parmi notre ensemble important de colonnes, que celles\n",
        "qui ont une importance non nulle.\n",
        "\n",
        "<div class=\"alert alert-success\" role=\"alert\">\n",
        "<h3 class=\"alert-heading\"><i class=\"fa-solid fa-pencil\"></i> Exercice 4 : Compréhension du modèle</h3>\n",
        "\n",
        "1.  Récupérer la *feature importance* directement depuis la couche adaptée de votre *pipeline*\n",
        "2.  Utiliser le code suivant pour calculer l’intervalle de confiance de cette mesure d’importance:\n",
        "\n",
        "``` python\n",
        "std = np.std([tree.feature_importances_ for tree in pipe['randomforest'].estimators_], axis=0)\n",
        "```\n",
        "\n",
        "1.  Représenter les variables d’importance non nulle. Qu’en concluez-vous ?\n",
        "\n",
        "</div>\n",
        "\n",
        "Le graphique d’importance des variables que vous devriez obtenir à l’issue\n",
        "de cet exercice est le suivant."
      ],
      "id": "daf2a7b8-aee3-409b-b87b-69ad6d6e0739"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "features_names = pipe[:-1].get_feature_names_out()\n",
        "importances = pipe['randomforest'].feature_importances_\n",
        "std = np.std([tree.feature_importances_ for tree in pipe['randomforest'].estimators_], axis=0)\n",
        "\n",
        "forest_importances = pd.DataFrame(importances, index=features_names, columns = [\"mdi\"])\n",
        "forest_importances['std'] = std\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "forest_importances.loc[forest_importances['mdi']>0, 'mdi'].plot.bar(\n",
        "    yerr = forest_importances.loc[forest_importances['mdi']>0, 'std'], ax = ax\n",
        ")\n",
        "ax.set_title(\"Feature importances using MDI\")\n",
        "ax.set_ylabel(\"Mean decrease in impurity\")\n",
        "fig.tight_layout()"
      ],
      "id": "b15cdbed"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Les statistiques obtenues par le biais de cette *variable importance*\n",
        "sont un peu rudimentaires mais permettent déjà de comprendre\n",
        "le problème de notre modèle.\n",
        "\n",
        "On voit donc que deux de nos variables déterminantes sont des effets fixes\n",
        "géographiques (qui servent à ajuster de la différence de prix entre\n",
        "Paris et les Hauts de Seine et le reste de la France), une autre variable\n",
        "est un effet fixe type de bien. Les deux variables qui pourraient introduire\n",
        "de la variabilité, à savoir la surface et, dans une moindre mesure, le\n",
        "nombre de lots, ont une importance moindre.\n",
        "\n",
        "<div class=\"alert alert-info\" role=\"alert\">\n",
        "<h3 class=\"alert-heading\"><i class=\"fa-solid fa-comment\"></i> Note</h3>\n",
        "\n",
        "Idéalement, on utiliserait `Yellowbrick` pour représenter l’importance des variables\n",
        "Mais en l’état actuel du *pipeline* on a beaucoup de variables dont le poids\n",
        "est nul qui viennent polluer la visualisation. Vous pouvez\n",
        "consulter la\n",
        "[documentation de `Yellowbrick` sur ce sujet](https://www.scikit-yb.org/en/latest/api/model_selection/importances.html)\n",
        "\n",
        "</div>"
      ],
      "id": "4a1f7bab-127a-41a6-978f-797322a103f7"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "compar = pd.DataFrame([y_test, pipe.predict(X_test)]).T\n",
        "compar.columns = ['obs','pred']\n",
        "compar['diff'] = compar.obs - compar.pred"
      ],
      "id": "dfc3f304"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Les prédictions peuvent nous suggérer également\n",
        "qu’il y a un problème."
      ],
      "id": "00e4e3be-0534-43a4-98f6-f169c98cd82c"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "g = sns.relplot(data = compar, x = 'obs', y = 'pred', color = \"royalblue\", alpha = 0.8)\n",
        "g.set(ylim=(0, 2e6), xlim=(0, 2e6),\n",
        "      title='Evaluating estimation error on test sample',\n",
        "      xlabel='Observed values',\n",
        "      ylabel='Predicted values')\n",
        "g.ax.axline(xy1=(0, 0), slope=1, color=\"red\", dashes=(5, 2))"
      ],
      "id": "d5a7547f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Restriction du champ du modèle\n",
        "\n",
        "Mettre en oeuvre un bon modèle de prix au niveau France entière\n",
        "est complexe. Nous allons donc nous restreindre au champ suivant:\n",
        "les appartements dans Paris."
      ],
      "id": "372e4f9e-6733-4ff1-9407-5557b020b5cb"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "mutations_paris = mutations.drop(\n",
        "    colonnes_surface.tolist() + [\"Date mutation\", \"lprix\"], # ajouter \"confinement\" si données 2020\n",
        "    axis = \"columns\"\n",
        "    ).copy()\n",
        "\n",
        "mutations_paris = mutations_paris.loc[mutations_paris['Valeur fonciere'] < 5e6] #keep only values below 5 millions\n",
        "\n",
        "mutations_paris.columns = mutations_paris.columns.str.replace(\" \", \"_\")\n",
        "mutations_paris  = mutations_paris.dropna(subset = ['dep','Code_type_local','month'])\n",
        "mutations_paris = mutations_paris.loc[mutations_paris['dep'] == \"75\"]\n",
        "mutations_paris = mutations_paris.loc[mutations_paris['Code_type_local'] == 2].drop(['dep','Code_type_local'], axis = \"columns\")\n",
        "mutations_paris.loc[mutations_paris['surface']>0]"
      ],
      "id": "7f9df545"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-success\" role=\"alert\">\n",
        "<h3 class=\"alert-heading\"><i class=\"fa-solid fa-pencil\"></i> Exercice 4 : Pipeline plus simple</h3>\n",
        "\n",
        "Reprendre les codes précédents et reconstruire notre *pipeline* sur\n",
        "la nouvelle base en mettant en oeuvre une méthode de *boosting*\n",
        "plutôt qu’une forêt aléatoire.\n",
        "\n",
        "*La correction de cet exercice est apparente pour simplifier les prochaines étapes mais essayez de faire celui-ci de vous-même*.\n",
        "\n",
        "</div>\n",
        "\n",
        "A l’issue de cet exercice, vous devriez avoir des *MDI* proches\n",
        "de celles-ci :"
      ],
      "id": "c9e70863-9f09-4f64-bf7b-8970a4741085"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.pipeline import make_pipeline, Pipeline\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "mutations_paris = mutations.drop(\n",
        "    colonnes_surface.tolist() + [\"Date mutation\", \"lprix\"], # ajouter \"confinement\" si données 2020\n",
        "    axis = \"columns\"\n",
        "    ).copy()\n",
        "\n",
        "mutations_paris = mutations_paris.loc[mutations_paris['Valeur fonciere'] < 5e6] #keep only values below 5 millions\n",
        "\n",
        "mutations_paris.columns = mutations_paris.columns.str.replace(\" \", \"_\")\n",
        "mutations_paris  = mutations_paris.dropna(subset = ['dep','Code_type_local','month'])\n",
        "mutations_paris = mutations_paris.loc[mutations_paris['dep'] == \"75\"]\n",
        "mutations_paris = mutations_paris.loc[mutations_paris['Code_type_local'] == 2].drop(['dep','Code_type_local', 'Nombre_de_lots'], axis = \"columns\")\n",
        "mutations_paris.loc[mutations_paris['surface']>0]\n",
        "\n",
        "\n",
        "numeric_features = mutations_paris.columns[~mutations_paris.columns.isin(['month', 'Valeur_fonciere'])].tolist()\n",
        "categorical_features = ['month']\n",
        "\n",
        "reg = GradientBoostingRegressor(random_state=0)\n",
        "\n",
        "numeric_pipeline = make_pipeline(\n",
        "  SimpleImputer(),\n",
        "  StandardScaler()\n",
        ")\n",
        "transformer = make_column_transformer(\n",
        "    (numeric_pipeline, numeric_features),\n",
        "    (OneHotEncoder(sparse_output = False, handle_unknown = \"ignore\"), categorical_features))\n",
        "pipe = Pipeline(steps=[('preprocessor', transformer),\n",
        "                      ('boosting', reg)])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    mutations_paris.drop(\"Valeur_fonciere\", axis = 1),\n",
        "    mutations_paris[[\"Valeur_fonciere\"]].values.ravel(),\n",
        "    test_size = 0.2, random_state = 123\n",
        ")\n",
        "\n",
        "pipe.fit(X_train, y_train)\n",
        "\n",
        "pd.DataFrame(\n",
        "    pipe[\"boosting\"].feature_importances_, \n",
        "    index = pipe[:-1].get_feature_names_out()\n",
        ")"
      ],
      "id": "estim-model-paris"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Recherche des hyperparamètres optimaux avec une validation croisée\n",
        "\n",
        "On détecte que le premier modèle n’est pas très bon et ne nous aidera\n",
        "pas vraiment à évaluer de manière fiable l’appartement de nos rêves.\n",
        "\n",
        "On va essayer de voir si notre modèle ne serait pas meilleur avec des\n",
        "hyperparamètres plus adaptés. Après tout, nous avons choisi par défaut\n",
        "la profondeur de l’arbre mais c’était un choix au doigt mouillé.\n",
        "\n",
        "❓️ Quels sont les hyperparamètres qu’on peut essayer d’optimiser ?"
      ],
      "id": "21b686e1-19d2-4fd2-ba55-063931f049f4"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipe['boosting'].get_params()"
      ],
      "id": "bff17394"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Un [détour par la documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\n",
        "nous aide à comprendre ceux sur lesquels on va jouer. Par exemple, il serait\n",
        "absurde de jouer sur le paramètre `random_state` qui est la racine du générateur\n",
        "pseudo-aléatoire."
      ],
      "id": "9ece0b8d-0802-493d-bba7-3e3235db28e8"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = pd.concat((X_train, X_test), axis=0)\n",
        "Y = np.concatenate([y_train,y_test])"
      ],
      "id": "ef886919"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nous allons nous contenter de jouer sur les paramètres:\n",
        "\n",
        "-   `n_estimators`: Le nombre d’arbres de décision que notre forêt contient\n",
        "-   `max_depth`: La profondeur de chaque arbre\n",
        "\n",
        "Il existe plusieurs manières de faire de la validation croisée. Nous allons ici\n",
        "utiliser la *grid search* qui consiste à estimer et tester le modèle sur chaque\n",
        "combinaison d’une grille de paramètres et sélectionner le couple de valeurs\n",
        "des hyperparamètres amenant à la meilleure prédiction. Par défaut, `scikit`\n",
        "effectue une *5-fold cross validation*. Nous n’allons pas changer\n",
        "ce comportement.\n",
        "\n",
        "Comme expliqué précédemment, les paramètres s’appelent sous la forme\n",
        "`<step>__<parameter_name>`\n",
        "\n",
        "La validation croisée pouvant être très consommatrice de temps, nous\n",
        "n’allons l’effectuer que sur un nombre réduit de valeurs de notre grille.\n",
        "Il est possible de passer la liste des valeurs à passer au crible sous\n",
        "forme de liste\n",
        "(comme nous allons le proposer pour l’argument `max_depth` dans l’exercice ci-dessous) ou\n",
        "sous forme d’`array` (comme nous allons le proposer pour l’argument `n_estimators`) ce qui est\n",
        "souvent pratique pour générer un criblage d’un intervalle avec `np.linspace`.\n",
        "\n",
        "<div class=\"alert alert-success\" role=\"alert\">\n",
        "<h3 class=\"alert-heading\"><i class=\"fa-solid fa-lightbulb\"></i> Tip</h3>\n",
        "\n",
        "Les estimations sont, par défaut, menées de manière séquentielle (l’une après\n",
        "l’autre). Nous sommes cependant face à un problème\n",
        "*embarassingly parallel*.\n",
        "Pour gagner en performance, il est recommandé d’utiliser l’argument\n",
        "`n_jobs=-1`.\n",
        "\n",
        "</div>"
      ],
      "id": "b86e7f47-7108-4ce2-8f6b-c4a57b20ff18"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "# Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
        "param_grid = {\n",
        "    \"boosting__n_estimators\": np.linspace(5,25, 5).astype(int),\n",
        "    \"boosting__max_depth\": [2,4]\n",
        "}\n",
        "grid_search = GridSearchCV(pipe, param_grid=param_grid)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"Elapsed time : {int(end_time - start_time)} seconds\")"
      ],
      "id": "grid-search"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_search"
      ],
      "id": "24515889"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_search.best_params_\n",
        "grid_search.best_estimator_"
      ],
      "id": "1056b547"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Toutes les performances sur les ensembles d’échantillons et de test sur la grille\n",
        "d’hyperparamètres sont disponibles dans l’attribut:"
      ],
      "id": "6411d8f8-d463-4d03-a7e3-0af1b327cace"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "perf_random_forest = pd.DataFrame(grid_search.cv_results_)"
      ],
      "id": "d2a10310"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Regardons les résultats moyens pour chaque valeur des hyperparamètres:"
      ],
      "id": "266d599e-4afa-4e78-9517-ae9063f1e8a3"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1)\n",
        "g = sns.lineplot(data = perf_random_forest, ax = ax,\n",
        "             x = \"param_boosting__n_estimators\",\n",
        "             y = \"mean_test_score\",\n",
        "             hue = \"param_boosting__max_depth\")\n",
        "g.set(xlabel='Number of estimators', ylabel='Mean score on test sample')\n",
        "g\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0,\n",
        "           title='Depth of trees')"
      ],
      "id": "97674edb"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Globalement, à profondeur d’arbre donnée, le nombre d’arbres affecte\n",
        "la performance. Changer la profondeur de l’arbre améliore la\n",
        "performance de manière plus marquée.\n",
        "\n",
        "Maintenant, il nous reste à re-entraîner le modèle avec ces nouveaux\n",
        "paramètres sur l’ensemble du jeu de *train* et l’évaluer sur l’ensemble\n",
        "du jeu de *test* :"
      ],
      "id": "526a1bbd-9069-4421-98fc-5e5a147e6639"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipe_optimal = grid_search.best_estimator_\n",
        "pipe_optimal.fit(X_train, y_train)\n",
        "\n",
        "compar = pd.DataFrame([y_test, pipe_optimal.predict(X_test)]).T\n",
        "compar.columns = ['obs','pred']\n",
        "compar['diff'] = compar.obs - compar.pred"
      ],
      "id": "525af205"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On obtient le RMSE suivant :"
      ],
      "id": "0c385c27-9e40-455d-9b71-5dcd3320ec29"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Le RMSE sur le jeu de test est {:,}\".format(\n",
        "   int(np.sqrt(mean_squared_error(y_test, pipe_optimal.predict(X_test))))\n",
        "))"
      ],
      "id": "9fd32889"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Et si on regarde la qualité en prédiction:\n",
        "\n",
        "On obtient plus de variance dans la prédiction, c’est déjà un peu mieux.\n",
        "Cependant, cela reste décevant pour plusieurs raisons:\n",
        "\n",
        "-   nous n’avons pas fait d’étape de sélection de variable\n",
        "-   nous n’avons pas chercher à déterminer si la variable à prédire la plus\n",
        "    pertinente était le prix ou une transformation de celle-ci\n",
        "    (par exemple le prix au $m^2$)\n",
        "\n",
        "# 6. Prochaine étape\n",
        "\n",
        "Nous avons un modèle certes perfectible mais fonctionnel.\n",
        "La question qui se pose maintenant c’est d’essayer d’en faire\n",
        "quelque chose au service des utilisateurs. Cela nous amène vers\n",
        "la question de la **mise en production**.\n",
        "\n",
        "Ceci est l’objet du prochain chapitre. Il s’agira d’une version introductive\n",
        "des enjeux évoqués dans le cadre du cours de\n",
        "3e année de [mise en production de projets de *data science*](https://ensae-reproductibilite.github.io/website/).\n",
        "\n",
        "# Références\n",
        "\n",
        "Breiman, Leo. 1996. « Bagging predictors ». *Machine learning* 24: 123‑40.\n",
        "\n",
        "———. 2001. « Random forests ». *Machine learning* 45: 5‑32."
      ],
      "id": "fd2a32a5-3a85-4fbe-ba78-e339198e7bf9"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "/opt/conda/share/jupyter/kernels/python3"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  }
}